# Equality

&mdash; [_Deux Conceptions du Racisme_ by Justin Veilleux](20230407164617-deux_conceptions_du_raisme.pdf)

&mdash; <https://youtu.be/aMcjxSThD54>

&mdash; <https://english.stackexchange.com/questions/100576/whats-the-difference-between-just-and-fair>

[[equality]] may be taken to refer to _fairness_ or to _justice_ &mdash; me. _fairness_ treats people identically, regardless of circumstances, while _justice_ treats people differently, owing to circumstances

> **example** a family rations food. a _fair_ parent gives each child the same amount of food, impartially and free of bias. a _just_ parent gives more food to the hungrier or bigger child, so as to best satisfy the needs of each child

additionally, [[equality]] can mean both _equality of process_ and _equality of outcome_ &mdash; me; assuming an inequal starting point, they are mutually exclusive. consequently, a inequal outcome does not imply an inequal process and an equal process does not imply an equal outcome

> **example** it can be argued that gender is a [[proxy]] to a myriad of characteristics that cause more men than women to be CEOs; it is an exmple of _outcome in[[equality]]_ but is not a proof of _process in[[equality]]_. moreover, _process [[equality]]_ would not guarantee _outcome [[equality]]_ because of fundamental intrinsic differences between the genders &mdash; me and <https://youtu.be/aMcjxSThD54?t=331> and <https://youtu.be/aMcjxSThD54?t=725>

given as only information a few [[proxy]]es, the _optimal_ process for the [[optimization]] of a given metric is likely to be _unfair_. consequently, _fair_ processes may lead to _suboptimal_ results

> **example** let's assume more men than women are engineers; that's the [[proxy]]. given pictures of people, an _unfair_ yet arguably _optimal_ process to guess which people are engineers is to only select the men. a _fair_ process could consist of selecting people at random; the _fair_ process would lead to a _suboptimal_ result
