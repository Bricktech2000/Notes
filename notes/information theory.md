# Information Theory

&mdash; <https://www.youtube.com/@JakobFoerster/videos>

&mdash; <https://youtu.be/B3y0RsVCyrw>

**see**

[[data compression]]

[[error correction]]

[[information content]]

[[information entropy]]

#todo

weighing game, information content, entropy (lecture 3 start)

source coding theorem (lecture 3 43:59, lecture 4 start)

maybe rewatch lecture 4

idea: use [[huffman coding]] [[algorithm]] at every character (or token) of a message where probabilities are generated by a LLM. compare [[data compression]] ratio with [[entropy]] of english <https://cs.stanford.edu/people/eroberts/courses/soco/projects/1999-00/information-theory/entropy_of_english_9.html>
